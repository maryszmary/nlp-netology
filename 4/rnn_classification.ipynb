{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JxouBo-Tf6m",
        "colab_type": "text"
      },
      "source": [
        "# Анализ тональности с pytorch и RNN\n",
        "\n",
        "[Этот ноутбук в colab](https://colab.research.google.com/drive/1VWAERpPsozIc2hVQ5WdA_ilI0tDQ4di5)\n",
        "\n",
        "[источник](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)\n",
        "\n",
        "## Рекуррентная нейросеть\n",
        "\n",
        "Проходится по последовательности (например, слов)  и на каждом шаге получает вектор слова и вектор предыдущего состояния.\n",
        "\n",
        "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
        "\n",
        "![rnn.png](https://miro.medium.com/max/627/1*go8PHsPNbbV6qRiwpUQ5BQ.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ8l8Vk3TX41",
        "colab_type": "text"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Мы будем использовать объекты класса `Field`. Они определяют, как данные будут храниться и обрабатываться.\n",
        "\n",
        "В поле `TEXT` задаём `tokenize='spacy'`. Это определяет, что тексты будут токенизироваться с помошью [spaCy](https://spacy.io) tokenizer. Если не подать аргументов, тексты будут разбиваться по пробелам.\n",
        "\n",
        "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels. We will explain the `dtype` argument later.\n",
        "\n",
        "Больше про класс `Field` [здесь](https://github.com/pytorch/text/blob/master/torchtext/data/field.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRTlfiN8JfOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 1234 # фиксируем seed для воспроизводимости\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xSFShnFJfOu",
        "colab_type": "text"
      },
      "source": [
        "В pytorch (`torchtext.datasets`) хранятся некоторые стандартные датасеты. А ещё в них встроено разделение на train и test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsHpQRUyJfOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6sR-kNKtHKX",
        "colab_type": "code",
        "outputId": "545fbcf3-ecd9-4fa1-e766-f1e0e1c55037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz:   0%|          | 180k/84.1M [00:00<00:52, 1.60MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 72.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UZImWWqJfOz",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим сколько примеров в каждой части:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8BxQNeGJfO0",
        "colab_type": "code",
        "outputId": "f9ee80e2-8b13-432d-b012-bb0898aeac8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIaWOUXmJfO6",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на один пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlDTg2UWJfO8",
        "colab_type": "code",
        "outputId": "9d64a390-3fcd-426e-a712-1639d7693702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Loved', 'it', '!', 'What', \"'s\", 'not', 'to', 'like?--you', 'got', 'your', 'suburbia', ',', 'you', 'got', 'your', 'zombies', ',', 'you', 'got', 'your', 'family', 'issues', ',', 'you', 'got', 'your', 'social', 'dilemmas', ',', 'you', 'got', 'yourself', 'one', 'Fine', \"Retro-1950's\", '-', 'style', 'Flesh', 'Eating', 'Under', 'Class', 'Held', 'At', 'Bay', 'By', 'An', 'Uneasy', 'Worried', 'About', 'Whether', 'They', \"'re\", 'The', 'Next', 'Meal', 'Upper', 'Crust', '.', 'You', 'could', \"n't\", 'ask', 'for', 'more.<br', '/><br', '/>Cast', 'is', 'superb', '.', 'Carrie', 'Ann', 'Moss', 'is', 'absolute', 'perfection', 'as', 'a', 'debutante', 'social', 'climbing', 'housewife', '.', 'She', \"'s\", 'both', 'wanton', ',', 'and', 'criminally', 'conspiratorial', '.', 'Every', 'fellow', \"'s\", 'dream', '.', \"K'sun\", 'is', 'really', 'great', 'as', 'the', 'son', 'just', 'trying', 'to', 'be', 'as', 'normal', 'as', 'possible', 'in', 'this', 'nightmare', 'existence', ',', 'and', 'somehow', 'succeeding', '.', 'He', \"'s\", 'a', 'genuine', 'screen', 'presence', '.', 'Very', 'photogenic', ',', 'and', 'natural', '.', 'Without', 'naming', 'them', 'all', ',', 'the', 'rest', 'of', 'the', 'cast', 'is', 'wonderful', '.', 'Henry', 'Czerny', 'plays', 'a', 'suspicious', 'policeman', 'with', 'honed', 'instincts', 'and', 'little', 'squeamishness', 'as', 'if', 'it', \"'s\", 'his', 'everyday', 'persona', '.', 'Billy', 'Connolly', 'is', 'delightful', 'as', 'Fido', '.', 'A', 'fine', 'actor', ':', 'I', 'wish', 'that', 'he', 'had', 'played', 'the', 'title', 'role', 'in', '\"', 'Braveheart', ',', '\"', 'with', 'Gibson', 'directing', '.', 'My', 'sense', 'is', 'that', 'his', 'William', 'Wallace', 'would', 'have', 'been', 'closer', 'to', 'the', 'actual', 'character', '.', 'His', 'Fido', 'is', 'contained', ',', 'yet', 'accessible', '.', 'A', 'nice', 'touch.<br', '/><br', '/>In', 'short', ',', 'a', 'great', 'and', 'marvelous', 'satiric', 'poke', 'at', 'morals', ',', 'values', ',', 'social', 'pinnings', ',', 'feelings', ',', 'growth', ',', 'coping', 'in', 'uncertain', 'times', ',', 'and', 'compensatory', 'adjustments', 'to', 'impossible', 'conditions', '.', 'A', 'true', 'reality', 'show', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmvUgT-toF0",
        "colab_type": "code",
        "outputId": "8e253f95-ca55-4fab-bd89-bf79c369c6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data.examples[0].text[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Loved', 'it', '!', 'What', \"'s\", 'not', 'to', 'like?--you', 'got', 'your']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Goav1LcnvlnP",
        "colab_type": "code",
        "outputId": "2baff434-c642-4a9a-f673-43bc4c0f6bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data.examples[0].label"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKdLuzhSJfPB",
        "colab_type": "text"
      },
      "source": [
        "Создадим валидационную выборку с помощью метода `.split()`. Соотношение количества примеров можно задать с помощью аргумента `split_ratio`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLYj80cNvW5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNV1Yf5lJfPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk110lvmJfPH",
        "colab_type": "text"
      },
      "source": [
        "Again, we'll view how many examples are in each split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khoVR97-JfPJ",
        "colab_type": "code",
        "outputId": "b6e76b81-b6aa-41a3-a3e9-09c527dfacbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t9caVm5JfPN",
        "colab_type": "text"
      },
      "source": [
        "Теперь построим _vocabulary_. Это, по большому счёту, таблица, в которой каждое слово соответствует индексу. Индекс используется для создания _one-hot_ векторов.\n",
        "\n",
        "![](assets/sentiment5.png)\n",
        "\n",
        "Количество уникальных слов 100,000 -- это очень много для векторов. Столько данных может не влезть в GPU. Поэтому мы возьмём 25,000 самых частых слов. Если нам встретится слово, которого среди них нет, оно заменится на  `<unk>`. Например, если в предожении \"This film is great and I love it\" не войдёт слово \"love\", получится \"This film is great and I `<unk>` it\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4hUIUkBJfPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSC5oisXJfPS",
        "colab_type": "text"
      },
      "source": [
        "Немного посмотрим на данные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFTerPCkJfPT",
        "colab_type": "code",
        "outputId": "ba387dd2-3957-44d8-d116-f403410a275d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfRXgFR2JfPb",
        "colab_type": "text"
      },
      "source": [
        "25002, а не 25000 -- из-за `<unk>` и `<pad>` токенов.\n",
        "\n",
        "В батче должны быть однаковые длины предложений, если какое-то короче -- добавляем паддинг.\n",
        "\n",
        "![](assets/sentiment6.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckse79eLJfPc",
        "colab_type": "code",
        "outputId": "a9ff0e97-0bb6-4fab-f0a4-80b74ae638e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 201964), (',', 192194), ('.', 165535), ('and', 109744), ('a', 109057), ('of', 100617), ('to', 93468), ('is', 76658), ('in', 61031), ('I', 54486), ('it', 53713), ('that', 49465), ('\"', 44201), (\"'s\", 43145), ('this', 42353), ('-', 36686), ('/><br', 35729), ('was', 34929), ('as', 30116), ('with', 29940)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN5JgIgdJfPf",
        "colab_type": "text"
      },
      "source": [
        "Данные в словаре: `stoi` (**s**tring **to** **i**nt) и `itos` (**i**nt **to**  **s**tring)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksnBcsFJfPh",
        "colab_type": "code",
        "outputId": "4f60d293-931d-4479-9125-d5d35d8ca9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgT3eHFKJfPo",
        "colab_type": "code",
        "outputId": "a9f32123-5694-4b49-cebb-010d028973b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(TEXT.vocab.stoi['and'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JD2WAkbJfPt",
        "colab_type": "text"
      },
      "source": [
        "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "We'll use a `BucketIterator` which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using `torch.device`, we then pass this device to the iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qisvSsHOJfPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1KSEGbjJfPy",
        "colab_type": "text"
      },
      "source": [
        "## Определяем модель\n",
        "\n",
        "У нас три слоя:\n",
        "* слой _embedding_ -- transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). Это полносвязный слой.\n",
        "* RNN (takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.)\n",
        "* слой _linear_ (transforming it to the correct output dimension)\n",
        "\n",
        "![RNN](https://raw.githubusercontent.com/bentrevett/pytorch-sentiment-analysis/9210842371c3bbde7b2007051dafa4c74d9768cd/assets/sentiment7.png)\n",
        "\n",
        "\n",
        "Each batch, `text`, is a tensor of size _**[sentence length, batch size]**_. That is a batch of sentences, each having each word converted into a one-hot vector. \n",
        "\n",
        "You may notice that this tensor should have another dimension due to the one-hot vectors, however PyTorch conveniently stores a one-hot vector as it's index value, i.e. the tensor representing a sentence is just a tensor of the indexes for each token in that sentence. The act of converting a list of tokens into a list of indexes is commonly called *numericalizing*.\n",
        "\n",
        "The input batch is then passed through the embedding layer to get `embedded`, which gives us a dense vector representation of our sentences. `embedded` is a tensor of size _**[sentence length, batch size, embedding dim]**_.\n",
        "\n",
        "`embedded` is then fed into the RNN. In some frameworks you must feed the initial hidden state, $h_0$, into the RNN, however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
        "\n",
        "The RNN returns 2 tensors, `output` of size _**[sentence length, batch size, hidden dim]**_ and `hidden` of size _**[1, batch size, hidden dim]**_. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. We verify this using the `assert` statement. Note the `squeeze` method, which is used to remove a dimension of size 1. \n",
        "\n",
        "Finally, we feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW3smrjnws62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq7bYBOIJfP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim) # слой, которым мы представляем слова в виде векторов \n",
        "\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim) # клетка RNN\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim) # ещё один слой в конце\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QBlTxShJfP6",
        "colab_type": "text"
      },
      "source": [
        "We now create an instance of our RNN class. \n",
        "\n",
        "The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size. \n",
        "\n",
        "The embedding dimension is the size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
        "\n",
        "The hidden dimension is the size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as on the vocabulary size, the size of the dense vectors and the complexity of the task.\n",
        "\n",
        "The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaVtI0QpJfP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-phyMen8JfP_",
        "colab_type": "text"
      },
      "source": [
        "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "475GxKBNJfQA",
        "colab_type": "code",
        "outputId": "40760ffc-96ba-462a-8cd7-744ce1779c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,592,105 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5wEZsBjJfQE",
        "colab_type": "text"
      },
      "source": [
        "## Обучаем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29vuOtfaJfQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWqVNmXZJfQP",
        "colab_type": "text"
      },
      "source": [
        "Определяем функцию потерь: _binary cross entropy with logits_. \n",
        "\n",
        "Our model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the _sigmoid_ or _logit_ functions. \n",
        "\n",
        "We then use this this bound scalar to calculate the loss using binary cross entropy. \n",
        "\n",
        "The `BCEWithLogitsLoss` criterion carries out both the sigmoid and the binary cross entropy steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va6CV6Q5JfQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INOVr_WlJfQT",
        "colab_type": "text"
      },
      "source": [
        "Using `.to`, we can place the model and the criterion on the GPU (if we have one). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IqlKN5aJfQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dgz83KTJfQa",
        "colab_type": "text"
      },
      "source": [
        "Функция для подсчёта accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K3pNWbyJfQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3owReSQJfQg",
        "colab_type": "text"
      },
      "source": [
        "The `train` function iterates over all examples, one batch at a time. \n",
        "\n",
        "`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Although we aren't using them in this model, it's good practice to include it.\n",
        "\n",
        "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
        "\n",
        "We then feed the batch of sentences, `batch.text`, into the model. Note, you do not need to do `model.forward(batch.text)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _**[batch size, 1]**_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _**[batch size]**_.\n",
        "\n",
        "The loss and accuracy are then calculated using our predictions and the labels, `batch.label`, with the loss being averaged over all examples in the batch.\n",
        "\n",
        "We calculate the gradient of each parameter with `loss.backward()`, and then update the parameters using the gradients and optimizer algorithm with `optimizer.step()`.\n",
        "\n",
        "The loss and accuracy is accumulated across the epoch, the `.item()` method is used to extract a scalar from a tensor which only contains a single value.\n",
        "\n",
        "Finally, we return the loss and accuracy, averaged across the epoch. The `len` of an iterator is the number of batches in the iterator.\n",
        "\n",
        "You may recall when initializing the `LABEL` field, we set `dtype=torch.float`. This is because TorchText sets tensors to be `LongTensor`s by default, however our criterion expects both inputs to be `FloatTensor`s. Setting the `dtype` to be `torch.float`, did this for us. The alternative method of doing this would be to do the conversion inside the `train` function by passing `batch.label.float()` instad of `batch.label` to the criterion. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o5pl1JWJfQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad() # обнуляем градиенты\n",
        "\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward() # считаем градиенты\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlLMprNoJfQk",
        "colab_type": "text"
      },
      "source": [
        "`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n",
        "\n",
        "`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Again, we are not using them in this model, but it is good practice to include them.\n",
        "\n",
        "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
        "\n",
        "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5WLm5S7JfQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRVfyROaJfQu",
        "colab_type": "text"
      },
      "source": [
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEj4QsqnJfQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7UjrXQIJfQz",
        "colab_type": "text"
      },
      "source": [
        "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets.\n",
        "\n",
        "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9EzB-3cJfQ0",
        "colab_type": "code",
        "outputId": "93a8288b-1351-43d9-f93c-4d106175d9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.93%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 49.84%\n",
            "Epoch: 02 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.01%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 49.88%\n",
            "Epoch: 03 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.08%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.52%\n",
            "Epoch: 04 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.81%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 49.40%\n",
            "Epoch: 05 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.98%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.41%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQx2EHMEJfQ8",
        "colab_type": "text"
      },
      "source": [
        "Loss уменьшается слабовато. Потому что эта модель далека от идеала. \n",
        "\n",
        "Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTY_ZeV_JfRD",
        "colab_type": "code",
        "outputId": "07042012-677e-4106-d847-9febb64be078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.711 | Test Acc: 47.01%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNQQuXRi0I2P",
        "colab_type": "text"
      },
      "source": [
        "## Используя предобученные эмбеддинги"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4snkIlg0S3A",
        "colab_type": "code",
        "outputId": "d0879e4e-67f4-47c2-a635-3b8e6592b104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:25, 2.23MB/s]                           \n",
            "100%|█████████▉| 398586/400000 [00:20<00:00, 19235.96it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i19O7uZs097v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu7u1fzK1BaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUwEbG0W1wOs",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на эмбеддинги:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QloFCs4m1oDS",
        "colab_type": "code",
        "outputId": "464ed554-7988-4504-8139-220c20268c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSX_Itjo1uYx",
        "colab_type": "code",
        "outputId": "b3c6dd38-b767-4a11-fee6-aa2dca4f47b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3159, -0.9579,  0.8782,  ...,  0.8522,  0.1121,  0.6375],\n",
              "        [-1.4131,  0.6201,  0.7174,  ...,  1.1835, -2.1611,  1.0010],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.4619,  0.1625, -0.1297,  ..., -0.1039, -0.4719,  0.7977],\n",
              "        [-0.3447, -0.3907,  0.4552,  ..., -0.5628,  0.2530,  0.4335],\n",
              "        [ 0.3796,  0.8294,  0.1816,  ...,  0.4350, -0.2332,  0.4504]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wag5DSA2v3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd0hjYFa22VE",
        "colab_type": "text"
      },
      "source": [
        "### Обучаем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvDTXTmA2wmD",
        "colab_type": "code",
        "outputId": "6586af79-ef59-41ae-c8d7-454938d1ea02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c1b298b1eeea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-517c6cf33def>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# обнуляем градиенты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b0427da8a844>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index -2 is out of bounds for dimension 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qSTzb_S272q",
        "colab_type": "code",
        "outputId": "88f5775f-4ab0-40b0-bb28-25e2eea95625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.706 | Test Acc: 37.54%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}